"""

The DNCM_encoder module is applied to the noise image generated by the diffusion model.

For more detailed information about the DNCM module, please refer to the following site: 
https://openaccess.thecvf.com/content/CVPR2023/papers/Ke_Neural_Preset_for_Color_Style_Transfer_CVPR_2023_paper.pdf

"""

from . import Backbone

import torch
import torch.nn as nn
import torch.nn.init as init
import torch.nn.functional as F

import torchvision

class DNCM_encoder(nn.Module):
    def __init__(self, k=16):
        super(DNCM_encoder, self).__init__()
        # Initialize hyper parameter K of classDNCM_encoder
        # Default value of k is 16
        self.k = k
        self.P = nn.Parameter(torch.randn((3, self.k)))  
        self.Q = nn.Parameter(torch.randn((self.k, 3)))

        init.xavier_uniform_(self.P)
        init.xavier_uniform_(self.Q)

        # Initialize encoder of class DNCM_encoder
        self.encoder = Backbone.VGG19()
        
        self.linear_d = None
        self.linear_r = None

    def forward(self, x):
        # VGG19
        x = self.encoder(x)

        if self.linear_d is None and self.linear_r is None:
            flattened_dim = x.view(x.size(0), -1).size(1)

            self.linear_d = nn.Linear(flattened_dim, self.k**2).to(x.device)
            self.linear_r = nn.Linear(flattened_dim, self.k**2).to(x.device)

        d = self.linear_d(x)
        d = d.view(-1, self.k, self.k)

        r = self.linear_r(x)
        r = r.view(-1, self.k, self.k)

        return d, r
    
    def downsample(self, x):
        return  F.interpolate(x, size=(256, 256), mode='bilinear', align_corners=False)
    
def encode(DNCM_encoder, image):
    downsampled_image = DNCM_encoder.downsample(image)
    d_image, r_image = DNCM_encoder(downsampled_image)

    return d_image, r_image

def DNCM(DNCM_encoder, content_image, style_image, option):
    batch_size, height, width =  content_image.shape[0], content_image.shape[2], content_image.shape[3]

    d_content, r_content = encode(DNCM_encoder, content_image)
    d_style, r_style = encode(DNCM_encoder, style_image)

    # (batch_size, 3, height, width) -> (batch_size, height*width, 3)
    reshaped_content_image = content_image.permute(0, 2, 3, 1).view(batch_size, height * width, 3)

    # (batch_size, height*width, 3) -> (batch_size, height*width, k)

    reshaped_content_image = torch.matmul(reshaped_content_image, DNCM_encoder.P)
    reshaped_content_image = torch.tanh(reshaped_content_image)

    # (batch_size, height*width, k) -> (batch_size, height*width, k)
    match option:
        case 'nDNCM':
            reshaped_content_image = torch.matmul(reshaped_content_image, d_content)
        case 'sDNCM':
            reshaped_content_image = torch.matmul(reshaped_content_image, r_style)
    
    reshaped_content_image = torch.tanh(reshaped_content_image)

    # Apply Tanh activation function without the last inner product
    # (batch_size, height*width, k) -> (batch_size, height*width, 3)
    reshaped_content_image = torch.matmul(reshaped_content_image, DNCM_encoder.Q)

    # (batch_size, height*width, 3) -> (batch_size, 3, height, width)
    reshaped_content_image = reshaped_content_image.view(batch_size, height, width, 3).permute(0, 3, 1, 2)

    return reshaped_content_image

def calculate_L_con(Zi, Zj):
    norm = torch.norm(Zi-Zj, p=2, dim=(1, 2, 3))

    return norm

def calculate_L_rec(Yi, Yj, Ii, Ij):
    norm_i = torch.norm(Yi-Ii, p=1, dim=(1, 2, 3))
    norm_j = torch.norm(Yj-Ij, p=1, dim=(1, 2, 3))

    return norm_i+norm_j